#!/bin/bash

# Wildtrack Dataset Download and HDFS Setup Script
echo "ðŸ“¥ Downloading Wildtrack Dataset and Setting up HDFS Storage..."

# Colors for output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color

print_status() {
    echo -e "${GREEN}[INFO]${NC} $1"
}

print_warning() {
    echo -e "${YELLOW}[WARNING]${NC} $1"
}

print_error() {
    echo -e "${RED}[ERROR]${NC} $1"
}

print_step() {
    echo -e "${BLUE}[STEP]${NC} $1"
}

# Check if infrastructure is running
print_step "Checking if infrastructure is running..."
if ! docker ps | grep -q namenode; then
    print_error "Hadoop infrastructure not running. Please run ./setup.sh first"
    exit 1
fi

# Create local temporary directory
TEMP_DIR="./temp_wildtrack"
HDFS_BASE_PATH="/surveillance/wildtrack"

print_step "Creating temporary download directory..."
mkdir -p $TEMP_DIR

# Download Wildtrack dataset (you'll need to modify the URL)
print_step "Downloading Wildtrack dataset..."
print_warning "Please ensure you have downloaded the Wildtrack dataset from Kaggle"
print_warning "Expected size: ~10GB"

# Check if dataset exists locally
if [ ! -d "./data/wildtrack" ] || [ -z "$(ls -A ./data/wildtrack)" ]; then
    print_warning "Dataset not found in ./data/wildtrack/"
    echo "Please download from: https://www.kaggle.com/datasets/your-wildtrack-dataset-link"
    echo "Extract to: ./data/wildtrack/"
    echo
    read -p "Have you downloaded and extracted the dataset to ./data/wildtrack/? (y/n): " confirm
    if [[ $confirm != [yY] ]]; then
        print_error "Please download the dataset first"
        exit 1
    fi
fi

# Verify dataset structure
print_step "Verifying dataset structure..."
required_files=(
    "data/wildtrack/cam1.mp4"
    "data/wildtrack/cam2.mp4" 
    "data/wildtrack/cam3.mp4"
    "data/wildtrack/cam4.mp4"
    "data/wildtrack/cam5.mp4"
    "data/wildtrack/cam6.mp4"
    "data/wildtrack/cam7.mp4"
    # "data/wildtrack/Image_subsets"
    # "data/wildtrack/annotations_positions"
    # "data/wildtrack/calibrations"
)

for file in "${required_files[@]}"; do
    if [ ! -e "$file" ]; then
        print_error "Missing required file/directory: $file"
        exit 1
    fi
done

print_status "âœ… Dataset structure verified"

# Wait for HDFS to be ready
print_step "Waiting for HDFS to be ready..."
sleep 10

# Clean up any existing surveillance data in HDFS
print_step "Cleaning up existing surveillance data in HDFS..."
docker exec namenode hdfs dfs -rm -r -f /surveillance 2>/dev/null || true
# docker exec namenode hdfs dfs -rm -r -f /user/spark 2>/dev/null || true
print_status "âœ… Existing data cleaned up"

# Create HDFS directories with proper permissions
print_step "Setting up HDFS directory structure with proper permissions..."

# Create base directories first
print_status "Creating base directories..."
docker exec namenode hdfs dfs -mkdir -p /user/spark
docker exec namenode hdfs dfs -mkdir -p /surveillance
docker exec namenode hdfs dfs -mkdir -p $HDFS_BASE_PATH

# Set proper permissions
print_status "Setting directory permissions..."
docker exec namenode hdfs dfs -chmod 755 /user
docker exec namenode hdfs dfs -chown spark:spark /user/spark
docker exec namenode hdfs dfs -chmod 755 /surveillance
docker exec namenode hdfs dfs -chmod 755 $HDFS_BASE_PATH

# Verify permissions were set correctly
if docker exec namenode hdfs dfs -ls / | grep -q "surveillance"; then
    print_status "âœ… Base directory permissions set correctly"
else
    print_error "Failed to set base directory permissions"
    exit 1
fi

# Create subdirectories for wildtrack data
print_status "Creating wildtrack subdirectories..."
docker exec namenode hdfs dfs -mkdir -p $HDFS_BASE_PATH/videos
# docker exec namenode hdfs dfs -mkdir -p $HDFS_BASE_PATH/images
# docker exec namenode hdfs dfs -mkdir -p $HDFS_BASE_PATH/annotations
# docker exec namenode hdfs dfs -mkdir -p $HDFS_BASE_PATH/calibrations

# Set permissions for subdirectories
docker exec namenode hdfs dfs -chmod 755 $HDFS_BASE_PATH/videos
# docker exec namenode hdfs dfs -chmod 755 $HDFS_BASE_PATH/images
# docker exec namenode hdfs dfs -chmod 755 $HDFS_BASE_PATH/annotations
# docker exec namenode hdfs dfs -chmod 755 $HDFS_BASE_PATH/calibrations

# Verify subdirectories were created
if docker exec namenode hdfs dfs -ls $HDFS_BASE_PATH | grep -q "videos"; then
    print_status "âœ… Subdirectories created successfully"
else
    print_error "Failed to create subdirectories"
    exit 1
fi

print_status "âœ… HDFS directory structure created with proper permissions"

# Verify directory structure and permissions
print_step "Verifying HDFS directory structure..."
docker exec namenode hdfs dfs -ls /
docker exec namenode hdfs dfs -ls /surveillance
docker exec namenode hdfs dfs -ls $HDFS_BASE_PATH

# Upload videos to HDFS
print_step "Uploading camera videos to HDFS..."
for i in {1..7}; do
    if [ -f "data/wildtrack/cam${i}.mp4" ]; then
        print_status "Uploading cam${i}.mp4..."
        docker exec namenode hdfs dfs -put /tmp/wildtrack/cam${i}.mp4 $HDFS_BASE_PATH/videos/ 2>/dev/null || \
        docker cp data/wildtrack/cam${i}.mp4 namenode:/tmp/ && \
        docker exec namenode hdfs dfs -put /tmp/cam${i}.mp4 $HDFS_BASE_PATH/videos/
    fi
done

# Upload Image_subsets to HDFS
# print_step "Uploading image subsets to HDFS..."
# if [ -d "data/wildtrack/Image_subsets" ]; then
#     docker cp data/wildtrack/Image_subsets namenode:/tmp/
#     docker exec namenode hdfs dfs -put /tmp/Image_subsets $HDFS_BASE_PATH/images/
# fi

# Upload annotations to HDFS
# print_step "Uploading annotations to HDFS..."
# if [ -d "data/wildtrack/annotations_positions" ]; then
#     docker cp data/wildtrack/annotations_positions namenode:/tmp/
#     docker exec namenode hdfs dfs -put /tmp/annotations_positions $HDFS_BASE_PATH/annotations/
# fi

# Upload calibrations to HDFS
# print_step "Uploading calibrations to HDFS..."
# if [ -d "data/wildtrack/calibrations" ]; then
#     docker cp data/wildtrack/calibrations namenode:/tmp/
#     docker exec namenode hdfs dfs -put /tmp/calibrations $HDFS_BASE_PATH/calibrations/
# fi

# Upload other files
# print_step "Uploading additional files..."
# if [ -f "data/wildtrack/rectangles.pom" ]; then
#     docker cp data/wildtrack/rectangles.pom namenode:/tmp/
#     docker exec namenode hdfs dfs -put /tmp/rectangles.pom $HDFS_BASE_PATH/
# fi

# Verify HDFS upload
print_step "Verifying HDFS upload..."
docker exec namenode hdfs dfs -ls -R $HDFS_BASE_PATH

# Get HDFS usage
print_step "Checking HDFS usage..."
docker exec namenode hdfs dfs -du -h $HDFS_BASE_PATH

# Cleanup temporary files in container
print_step "Cleaning up temporary files..."
# docker exec namenode rm -rf /tmp/Image_subsets /tmp/annotations_positions /tmp/calibrations /tmp/cam*.mp4 /tmp/rectangles.pom
docker exec namenode rm -rf /tmp/cam*.mp4


print_status "âœ… Wildtrack dataset successfully uploaded to HDFS!"
echo
echo "ðŸ“Š HDFS Structure with Permissions:"
echo "  â€¢ Base: /surveillance/wildtrack/ (755 permissions)"
echo "  â€¢ Videos: $HDFS_BASE_PATH/videos/ (cam1.mp4 to cam7.mp4)"
# echo "  â€¢ Images: $HDFS_BASE_PATH/images/Image_subsets/"
# echo "  â€¢ Annotations: $HDFS_BASE_PATH/annotations/annotations_positions/"
# echo "  â€¢ Calibrations: $HDFS_BASE_PATH/calibrations/"
echo "  â€¢ User directory: /user/spark/ (owned by spark:spark)"
echo
echo "ðŸ”— Access HDFS Web UI: http://localhost:9870"
echo "   Navigate to: Utilities â†’ Browse the file system â†’ /surveillance/wildtrack"
echo
print_step "Final HDFS verification:"
docker exec namenode hdfs dfs -ls /
docker exec namenode hdfs dfs -ls /surveillance
echo
print_status "Next steps:"
echo "  1. Update Kafka producers to read from HDFS"
echo "  2. Configure Spark streaming to process HDFS data"
echo "  3. Test the complete pipeline"
